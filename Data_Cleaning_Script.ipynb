{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b0ee0-0c73-44e5-9ab1-377e87efb1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the file path of the dataset:  C:\\\\Users\\\\raxshana.k\\\\Downloads\\\\dataAnalystJobsIndia_7th_July_2024.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Backup of the original dataset created.\n",
      "\n",
      "Options:\n",
      "1. Basic Operations\n",
      "2. Column-wise Operations\n",
      "3. Show DataFrame\n",
      "4. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an option:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic Operations:\n",
      "1. Display basic info\n",
      "2. Display head and tail of data\n",
      "3. Check missing values\n",
      "4. Check inconsistencies\n",
      "5. Return to Main Menu\n",
      "6. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an option:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                                          job_title  \\\n",
      "0              0    JP Morgan Chase - Client Data Analyst (4-8 yrs)   \n",
      "1              1                           Data Analyst - Bangalore   \n",
      "2              2  Senior IT ATLAS Data Analyst and Integration S...   \n",
      "3              3                            Consultant Data Analyst   \n",
      "4              4           Senior Data Analyst - Retail Liabilities   \n",
      "...          ...                                                ...   \n",
      "1556        1556                    Data Analyst / Sr. Data Analyst   \n",
      "1557        1557                  Artificial intelligence Architect   \n",
      "1558        1558                                      Data Analysts   \n",
      "1559        1559                                     Data Architect   \n",
      "1560        1560                                       Data analyst   \n",
      "\n",
      "                       company experience  min exp  max exp        salary  \\\n",
      "0              JP Morgan Chase    4-8 Yrs      4.0      8.0           NaN   \n",
      "1           Schneider Electric   5-10 Yrs      5.0     10.0  ₹ 8 - 16L/yr   \n",
      "2     SAP Labs India Pvt. Ltd.   7-11 Yrs      7.0     11.0           NaN   \n",
      "3                       Pfizer    2-6 Yrs      2.0      6.0           NaN   \n",
      "4              IDFC FIRST Bank   5-10 Yrs      5.0     10.0           NaN   \n",
      "...                        ...        ...      ...      ...           ...   \n",
      "1556            CoreInsightsAI    0-8 Yrs      0.0      8.0           NaN   \n",
      "1557               Cotocus.com    4-9 Yrs      4.0      9.0           NaN   \n",
      "1558               Cotocus.com    1-4 Yrs      1.0      4.0           NaN   \n",
      "1559               Cotocus.com   6-10 Yrs      6.0     10.0           NaN   \n",
      "1560                   Zecotok    1-3 Yrs      1.0      3.0           NaN   \n",
      "\n",
      "      base salary  max salary                                location  \\\n",
      "0             NaN         NaN                Hyderabad / Secunderabad   \n",
      "1        800000.0   1600000.0  Bangalore / Bengaluru, Bangalore Rural   \n",
      "2             NaN         NaN                   Bangalore / Bengaluru   \n",
      "3             NaN         NaN                                  Mumbai   \n",
      "4             NaN         NaN                                  Mumbai   \n",
      "...           ...         ...                                     ...   \n",
      "1556          NaN         NaN                                  Remote   \n",
      "1557          NaN         NaN                   Bangalore / Bengaluru   \n",
      "1558          NaN         NaN                   Bangalore / Bengaluru   \n",
      "1559          NaN         NaN                   Bangalore / Bengaluru   \n",
      "1560          NaN         NaN                               Panchkula   \n",
      "\n",
      "      jobListed(days ago) postedIn  rating  reviews count  \\\n",
      "0                     3.0  iimjobs     4.1         5300.0   \n",
      "1                     4.0   Naukri     4.2         3500.0   \n",
      "2                     7.0   Naukri     4.3         1500.0   \n",
      "3                     3.0   Naukri     4.1         1700.0   \n",
      "4                     5.0   Naukri     4.5         2300.0   \n",
      "...                   ...      ...     ...            ...   \n",
      "1556                150.0   Naukri     NaN            NaN   \n",
      "1557                365.0   Naukri     NaN            NaN   \n",
      "1558                365.0   Naukri     NaN            NaN   \n",
      "1559                365.0   Naukri     NaN            NaN   \n",
      "1560                180.0   Naukri     NaN            NaN   \n",
      "\n",
      "                                               details  \\\n",
      "0                     Banking, KYC, Banking Operations   \n",
      "1                              ERP, SAP, Data Analysis   \n",
      "2      data quality, Data analysis, quality management   \n",
      "3     Power Bi, Commercial Operations, Data Management   \n",
      "4        Test strategy, Analytical, Manager Technology   \n",
      "...                                                ...   \n",
      "1556             Data analysis, Analytical, Consulting   \n",
      "1557     Cloud computing, Machine learning, Analytical   \n",
      "1558  Analytical, Continuous improvement, Data Analyst   \n",
      "1559             Scrum, Data analysis, Data management   \n",
      "1560          Data analysis, Programming, Data Analyst   \n",
      "\n",
      "        salary data provide by  \n",
      "0                          NaN  \n",
      "1     Salary Listed by Company  \n",
      "2                          NaN  \n",
      "3                          NaN  \n",
      "4                          NaN  \n",
      "...                        ...  \n",
      "1556                       NaN  \n",
      "1557                       NaN  \n",
      "1558                       NaN  \n",
      "1559                       NaN  \n",
      "1560                       NaN  \n",
      "\n",
      "[1561 rows x 16 columns]\n",
      "\n",
      "Basic Information about the Dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1561 entries, 0 to 1560\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Unnamed: 0              1561 non-null   int64  \n",
      " 1   job_title               1561 non-null   object \n",
      " 2   company                 1561 non-null   object \n",
      " 3   experience              1527 non-null   object \n",
      " 4   min exp                 1527 non-null   float64\n",
      " 5   max exp                 1527 non-null   float64\n",
      " 6   salary                  343 non-null    object \n",
      " 7   base salary             343 non-null    float64\n",
      " 8   max salary              343 non-null    float64\n",
      " 9   location                1528 non-null   object \n",
      " 10  jobListed(days ago)     1561 non-null   float64\n",
      " 11  postedIn                1561 non-null   object \n",
      " 12  rating                  1124 non-null   float64\n",
      " 13  reviews count           1124 non-null   float64\n",
      " 14  details                 1528 non-null   object \n",
      " 15  salary data provide by  343 non-null    object \n",
      "dtypes: float64(7), int64(1), object(8)\n",
      "memory usage: 195.3+ KB\n",
      "None\n",
      "\n",
      "Basic Operations:\n",
      "1. Display basic info\n",
      "2. Display head and tail of data\n",
      "3. Check missing values\n",
      "4. Check inconsistencies\n",
      "5. Return to Main Menu\n",
      "6. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Choose an option:  2\n",
      "Would you like to see the first few rows (head), the last few rows (tail), or both? (head/tail/both):  head\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of the dataset:\n",
      "   Unnamed: 0                                          job_title  \\\n",
      "0           0    JP Morgan Chase - Client Data Analyst (4-8 yrs)   \n",
      "1           1                           Data Analyst - Bangalore   \n",
      "2           2  Senior IT ATLAS Data Analyst and Integration S...   \n",
      "3           3                            Consultant Data Analyst   \n",
      "4           4           Senior Data Analyst - Retail Liabilities   \n",
      "\n",
      "                    company experience  min exp  max exp        salary  \\\n",
      "0           JP Morgan Chase    4-8 Yrs      4.0      8.0           NaN   \n",
      "1        Schneider Electric   5-10 Yrs      5.0     10.0  ₹ 8 - 16L/yr   \n",
      "2  SAP Labs India Pvt. Ltd.   7-11 Yrs      7.0     11.0           NaN   \n",
      "3                    Pfizer    2-6 Yrs      2.0      6.0           NaN   \n",
      "4           IDFC FIRST Bank   5-10 Yrs      5.0     10.0           NaN   \n",
      "\n",
      "   base salary  max salary                                location  \\\n",
      "0          NaN         NaN                Hyderabad / Secunderabad   \n",
      "1     800000.0   1600000.0  Bangalore / Bengaluru, Bangalore Rural   \n",
      "2          NaN         NaN                   Bangalore / Bengaluru   \n",
      "3          NaN         NaN                                  Mumbai   \n",
      "4          NaN         NaN                                  Mumbai   \n",
      "\n",
      "   jobListed(days ago) postedIn  rating  reviews count  \\\n",
      "0                  3.0  iimjobs     4.1         5300.0   \n",
      "1                  4.0   Naukri     4.2         3500.0   \n",
      "2                  7.0   Naukri     4.3         1500.0   \n",
      "3                  3.0   Naukri     4.1         1700.0   \n",
      "4                  5.0   Naukri     4.5         2300.0   \n",
      "\n",
      "                                            details    salary data provide by  \n",
      "0                  Banking, KYC, Banking Operations                       NaN  \n",
      "1                           ERP, SAP, Data Analysis  Salary Listed by Company  \n",
      "2   data quality, Data analysis, quality management                       NaN  \n",
      "3  Power Bi, Commercial Operations, Data Management                       NaN  \n",
      "4     Test strategy, Analytical, Manager Technology                       NaN  \n",
      "\n",
      "Basic Operations:\n",
      "1. Display basic info\n",
      "2. Display head and tail of data\n",
      "3. Check missing values\n",
      "4. Check inconsistencies\n",
      "5. Return to Main Menu\n",
      "6. Exit\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from scipy.interpolate import interp1d, UnivariateSpline\n",
    "\n",
    "\n",
    "\n",
    "# Function to load the dataset\n",
    "def load_dataset():\n",
    "    try:\n",
    "        file_path = input(\"Enter the file path of the dataset: \").strip()\n",
    "        file_type = file_path.split('.')[-1].lower()\n",
    "\n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type in ['xls', 'xlsx']:\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_type == 'txt':\n",
    "            df = pd.read_csv(file_path, delimiter='\\t')  # Assumes tab-delimited text file\n",
    "        elif file_type == 'json':\n",
    "            df = pd.read_json(file_path)\n",
    "        else:\n",
    "            print(\"Unsupported file type. Please provide a CSV, Excel, TXT, or JSON file.\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(\"The specified file was not found. Please check the path and try again.\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"The file is empty. Please provide a valid file.\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"There was an error parsing the file. Please check the file format and content.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to create a backup of the original dataset\n",
    "def create_backup(df):\n",
    "    try:\n",
    "        backup_df = df.copy()\n",
    "        return backup_df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating a backup: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "#Function to display basic info\n",
    "def display_basic_info(df):\n",
    "    try:\n",
    "        print(\"\\nBasic Information about the Dataset:\")\n",
    "        print(df.info())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while displaying basic information: {e}\")\n",
    "\n",
    "\n",
    "#Function to display head and tail\n",
    "def show_head_and_tail(df, rows=5, display_choice='both'):\n",
    "    try:\n",
    "        if display_choice == 'head':\n",
    "            print(f\"\\nFirst {rows} rows of the dataset:\")\n",
    "            print(df.head(rows))\n",
    "        elif display_choice == 'tail':\n",
    "            print(f\"\\nLast {rows} rows of the dataset:\")\n",
    "            print(df.tail(rows))\n",
    "        elif display_choice == 'both':\n",
    "            print(f\"\\nFirst {rows} rows of the dataset:\")\n",
    "            print(df.head(rows))\n",
    "            print(f\"\\nLast {rows} rows of the dataset:\")\n",
    "            print(df.tail(rows))\n",
    "        else:\n",
    "            print(\"Invalid choice. Please select 'head', 'tail', or 'both'.\")\n",
    "    \n",
    "    except AttributeError as e:\n",
    "        print(\"Error: The provided object is not a valid DataFrame. Ensure it is properly initialized.\")\n",
    "        print(f\"Details: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "#Function to display missing value\n",
    "def check_missing_values(df):\n",
    "    try:\n",
    "        missing_info = df.isnull().sum()\n",
    "        missing_columns = missing_info[missing_info > 0]\n",
    "        if not missing_columns.empty:\n",
    "            print(\"\\nThe following columns contain missing values:\")\n",
    "            print(missing_columns)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking for missing values: {e}\")\n",
    " \n",
    "\n",
    "    \n",
    "#Function to check missing value\n",
    "def check_inconsistencies(df):\n",
    "    try:\n",
    "        print(\"\\nChecking for inconsistencies in numeric and categorical columns...\")\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    if (df[col] < 0).any() or df[col].isnull().any():\n",
    "                        print(f\"Column '{col}' may have inconsistencies such as negative values or nulls.\")\n",
    "                elif pd.api.types.is_string_dtype(df[col]):\n",
    "                    if df[col].str.contains('error', case=False, na=False).any():\n",
    "                        print(f\"Column '{col}' contains potential errors like 'error' values.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while checking column '{col}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking for inconsistencies: {e}\")\n",
    " \n",
    "\n",
    "\n",
    "#Function to remove columns\n",
    "def remove_columns_by_index(df):\n",
    "    try:\n",
    "        print(\"\\nCurrent columns in the dataset:\")\n",
    "        for index, col in enumerate(df.columns):\n",
    "            print(f\"{index}: {col}\")\n",
    "\n",
    "        remove_columns_indices = input(\"Do you want to remove any columns by index? (yes/no): \").strip().lower()\n",
    "        if remove_columns_indices == 'yes':\n",
    "            try:\n",
    "                indices_to_remove = input(\"Enter the column indices to remove, separated by commas: \").strip().split(',')\n",
    "                indices_to_remove = [int(index.strip()) for index in indices_to_remove]\n",
    "                columns_to_remove = [df.columns[i] for i in indices_to_remove if i < len(df.columns)]\n",
    "                if columns_to_remove:\n",
    "                    df.drop(columns=columns_to_remove, errors='ignore', inplace=True)\n",
    "                    print(f\"Columns {columns_to_remove} have been removed.\")\n",
    "                else:\n",
    "                    print(\"No valid column indices provided for removal.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter valid numeric indices.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while removing columns: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during column removal: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "#Function to drop duplicate\n",
    "def check_and_handle_duplicates(df1):\n",
    "    df = df1\n",
    "    try:\n",
    "        # Check for duplicates\n",
    "        duplicate_rows = df.duplicated().sum()\n",
    "        if duplicate_rows > 0:\n",
    "            print(f\"\\nYour dataset contains {duplicate_rows} duplicate rows.\")\n",
    "            \n",
    "            # Ask the user if they want to drop the duplicates\n",
    "            drop_choice = input(\"Do you want to drop these duplicate rows? (yes/no): \").strip().lower()\n",
    "            if drop_choice == 'yes':\n",
    "                print(f\"Initial shape of the DataFrame: {df.shape}\")\n",
    "                df.drop_duplicates(inplace=True)\n",
    "                print(f\"Shape after dropping duplicates: {df.shape}\")\n",
    "            else:\n",
    "                print(\"No duplicates were dropped.\")\n",
    "        else:\n",
    "            print(\"Your dataset contains no duplicate rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking or dropping duplicates: {e}\")\n",
    "        df = df1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#Function to drop missing rows\n",
    "def drop_missing_rows(df, subset):\n",
    "    try:\n",
    "        # Check for rows with missing values in the specified columns\n",
    "        missing_rows = df[df[subset].isnull().any(axis=1)]\n",
    "        \n",
    "        if not missing_rows.empty:\n",
    "            print(\"Rows with missing values in specified columns:\")\n",
    "            print(missing_rows)\n",
    "            \n",
    "            # Show a warning message\n",
    "            print(\"\\nWarning: Dropping rows with missing values may affect other columns.\")\n",
    "            \n",
    "            # Get the indices of the rows that have missing values\n",
    "            row_indices = missing_rows.index.tolist()\n",
    "            print(f\"Row indices with missing values: {row_indices}\")\n",
    "            \n",
    "            # Ask the user if they want to proceed with dropping these rows\n",
    "            user_input = input(\"Do you want to drop these rows? (yes/no): \").strip().lower()\n",
    "            if user_input == 'yes':\n",
    "                # Drop rows with missing values in the specified columns\n",
    "                cleaned_df = df.dropna(subset=subset)\n",
    "                print(f\"Rows with missing values in columns {subset} have been dropped.\")\n",
    "            else:\n",
    "                print(\"No rows were dropped.\")\n",
    "                cleaned_df = df\n",
    "        else:\n",
    "            print(\"No rows with missing values found in the specified columns.\")\n",
    "            cleaned_df = df\n",
    "        \n",
    "        return cleaned_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while dropping rows with missing values: {e}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "#Function to handle missing values\n",
    "def visualize_column_distribution(df: pd.DataFrame, column: str):\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "\n",
    "        # Histogram of non-missing values\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[column].dropna(), kde=True, bins=30)\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Frequency')\n",
    "\n",
    "        # Box plot of non-missing values\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[column].dropna())\n",
    "        plt.title(f'Box Plot of {column}')\n",
    "        plt.xlabel(column)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while visualizing column distribution: {e}\")\n",
    "\n",
    "def fill_numeric_missing_values(df: pd.DataFrame, column: str, strategy: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if strategy == 'mean':\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        elif strategy == 'median':\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "        elif strategy == 'mode':\n",
    "            df[column] = df[column].fillna(df[column].mode()[0])\n",
    "        elif strategy == 'zero':\n",
    "            df[column] = df[column].fillna(0)\n",
    "        elif strategy == 'forward':\n",
    "            df[column] = df[column].ffill()\n",
    "        elif strategy == 'backward':\n",
    "            df[column] = df[column].bfill()\n",
    "        elif strategy == 'whitespace':\n",
    "            df[column] = df[column].fillna('')\n",
    "        else:\n",
    "            print(f\"Unknown strategy '{strategy}' for numeric column '{column}'. Using mean as default.\")\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while filling missing numeric values in column '{column}': {e}\")\n",
    "    return df\n",
    "\n",
    "def fill_categorical_missing_values(df: pd.DataFrame, column: str, strategy: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if strategy == 'fill_value':\n",
    "            fill_value = input(f\"Enter a fill value for column '{column}' (e.g., 'Not available', 'Not disclosed'): \").strip()\n",
    "            df[column] = df[column].fillna(fill_value)\n",
    "        elif strategy == 'forward':\n",
    "            df[column] = df[column].astype(str).ffill()\n",
    "        elif strategy == 'backward':\n",
    "            df[column] = df[column].astype(str).bfill()\n",
    "        elif strategy == 'whitespace':\n",
    "            df[column] = df[column].astype(str).fillna('')\n",
    "        else:\n",
    "            print(f\"Unknown strategy '{strategy}' for categorical column '{column}'. Using 'whitespace' as default.\")\n",
    "            df[column] = df[column].astype(str).fillna('')\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while filling missing categorical values in column '{column}': {e}\")\n",
    "    return df\n",
    "\n",
    "def drop_column(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df.drop(columns=[column], inplace=True)\n",
    "        print(f\"Column '{column}' has been dropped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while dropping column '{column}': {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        # Identify columns with missing values\n",
    "        missing_info = df.isnull().sum()\n",
    "        columns_with_nan = missing_info[missing_info > 0]\n",
    "\n",
    "        if columns_with_nan.empty:\n",
    "            print(\"No columns with missing values.\")\n",
    "            return df\n",
    "\n",
    "        print(\"\\nColumns with missing values:\")\n",
    "        for index, (col, count) in enumerate(columns_with_nan.items()):\n",
    "            column_type = 'Numeric' if pd.api.types.is_numeric_dtype(df[col]) else 'Categorical'\n",
    "            print(f\"{index}: {col} ({column_type}) - count: {count}\")\n",
    "\n",
    "        selected_indices = input(\"Enter the indices of the columns you want to handle, separated by commas: \").strip().split(',')\n",
    "        selected_indices = [int(index.strip()) for index in selected_indices if index.strip().isdigit() and int(index.strip()) < len(columns_with_nan)]\n",
    "\n",
    "        selected_columns = [columns_with_nan.index[i] for i in selected_indices]\n",
    "\n",
    "        for col in columns_with_nan.index:\n",
    "            try:\n",
    "                if col in selected_columns:\n",
    "                    while True:\n",
    "                        print(f\"\\nProcessing column: '{col}'\")\n",
    "                        print(\"1. Drop column\")\n",
    "                        print(\"2. Handle missing values\")\n",
    "                        print(\"3. Exit\")\n",
    "                        choice = input(\"Choose an option: \").strip()\n",
    "                        if choice == '1':\n",
    "                            df = drop_column(df, col)\n",
    "                            break\n",
    "                        elif choice == '2':\n",
    "                            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                                visualize_column_distribution(df, col)  # Visualize the distribution\n",
    "                                strategy = input(f\"Column '{col}' is numeric with {missing_info[col]} missing values. Choose a strategy (mean/median/mode/zero/forward/backward/whitespace): \").strip().lower()\n",
    "                                df = fill_numeric_missing_values(df, col, strategy)\n",
    "                            else:\n",
    "                                strategy = input(f\"Column '{col}' is categorical with {missing_info[col]} missing values. Choose a strategy (fill_value/forward/backward/whitespace): \").strip().lower()\n",
    "                                df = fill_categorical_missing_values(df, col, strategy)\n",
    "                            break\n",
    "                        \n",
    "                        elif choice == '3':\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"Invalid choice. Please select 1, 2, 3, or 4.\")\n",
    "                else:\n",
    "                    # Apply default strategies for unselected columns\n",
    "                    print(f\"Filling unselected column '{col}' with default strategy.\")\n",
    "                    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                        df = fill_numeric_missing_values(df, col, 'zero')\n",
    "                    else:\n",
    "                        df = fill_categorical_missing_values(df, col, 'fill_value')\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while handling missing values in column '{col}': {e}\")\n",
    "\n",
    "        # Display the count of missing values after processing\n",
    "        print(\"\\nMissing values count after processing:\")\n",
    "        print(df.isnull().sum())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while handling missing values: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Function for interpolate_missing_values\n",
    "def visualize_data_pattern(df, column):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df[column], marker='o', linestyle='--', label='Original Data')\n",
    "    plt.title(f'Original Data with Missing Values in {column}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(column)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nUnderstanding the Nature of Your Data:\")\n",
    "    print(\"1. Linear Data: If your data follows a straight-line trend or you expect a linear relationship between data points, linear interpolation is generally sufficient.\")\n",
    "    print(\"2. Non-Linear Data: If your data has curves, peaks, or non-linear trends, a polynomial or spline interpolation might be more appropriate.\")\n",
    "    print(\"3. Smooth Continuous Data: If your data is expected to be smooth and continuous, such as in physical measurements or time series data, spline interpolation often works well.\")\n",
    "\n",
    "def plot_interpolation_comparison(df, column):\n",
    "    try:\n",
    "        # Original Data\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(df[column], marker='o', linestyle='--', label='Original Data')\n",
    "        plt.title('Original Data with Missing Values')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(column)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Linear Interpolation\n",
    "        df_linear = df.copy()\n",
    "        df_linear[column] = df_linear[column].interpolate(method='linear')\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(df_linear[column], marker='o', linestyle='-', color='r', label='Linear Interpolation')\n",
    "        plt.title('Linear Interpolation')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(column)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Polynomial Interpolation (2nd degree)\n",
    "        df_poly = df.copy()\n",
    "        poly_interp = interp1d(df.index[~df[column].isnull()], df[column].dropna(), kind='quadratic', fill_value='extrapolate')\n",
    "        df_poly[column] = poly_interp(df.index)\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(df_poly[column], marker='o', linestyle='-', color='g', label='Polynomial Interpolation (2nd Degree)')\n",
    "        plt.title('Polynomial Interpolation')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(column)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Spline Interpolation\n",
    "        df_spline = df.copy()\n",
    "        spline_interp = interp1d(df.index[~df[column].isnull()], df[column].dropna(), kind='cubic', fill_value='extrapolate')\n",
    "        df_spline[column] = spline_interp(df.index)\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(df_spline[column], marker='o', linestyle='-', color='b', label='Spline Interpolation')\n",
    "        plt.title('Spline Interpolation')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(column)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Based on the visualizations, consider the following:\")\n",
    "        print(\"1. Linear Interpolation: Useful for simple, linear trends.\")\n",
    "        print(\"2. Polynomial Interpolation: Good for capturing more complex trends.\")\n",
    "        print(\"3. Spline Interpolation: Useful for smooth, continuous data.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while plotting and suggesting interpolation methods: {e}\")\n",
    "\n",
    "def interpolate_missing_values(df):\n",
    "    try:\n",
    "        # Identify columns with missing values\n",
    "        missing_info = df.isnull().sum()\n",
    "        columns_with_nan = missing_info[missing_info > 0]\n",
    "\n",
    "        if columns_with_nan.empty:\n",
    "            print(\"No columns with missing values.\")\n",
    "            return df\n",
    "\n",
    "        print(\"\\nColumns with missing values:\")\n",
    "        for index, (col, count) in enumerate(columns_with_nan.items()):\n",
    "            print(f\"{index}: {col} - count: {count}\")\n",
    "\n",
    "        selected_index = int(input(\"Enter the index of the column you want to interpolate: \").strip())\n",
    "        column_to_interpolate = columns_with_nan.index[selected_index]\n",
    "\n",
    "        # Visualize data pattern and suggest methods\n",
    "        visualize_data_pattern(df, column_to_interpolate)\n",
    "\n",
    "        # Ask if the user wants to proceed with interpolation\n",
    "        proceed = input(f\"Do you want to perform interpolation on the column '{column_to_interpolate}'? (yes/no): \").strip().lower()\n",
    "        if proceed == 'yes':\n",
    "            plot_interpolation_comparison(df, column_to_interpolate)\n",
    "            method = input(f\"Choose an interpolation method for column '{column_to_interpolate}' (linear, polynomial, spline): \").strip().lower()\n",
    "\n",
    "            if method == 'linear':\n",
    "                df[column_to_interpolate] = df[column_to_interpolate].interpolate(method='linear')\n",
    "            elif method == 'polynomial':\n",
    "                df[column_to_interpolate] = df[column_to_interpolate].interpolate(method='polynomial', order=2)\n",
    "            elif method == 'spline':\n",
    "                df[column_to_interpolate] = df[column_to_interpolate].interpolate(method='spline', order=3)\n",
    "            else:\n",
    "                print(\"Invalid method selected. No interpolation performed.\")\n",
    "        else:\n",
    "            print(\"Interpolation skipped.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during interpolation: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Detect and Fix inaccurate data\n",
    "def detect_and_fix_inaccurate_data(df, column_index=None):\n",
    "    \"\"\"\n",
    "    Detect and fix inaccurate data in the DataFrame based on column index.\n",
    "    If column_index is None, process all columns.\n",
    "\n",
    "    Args:\n",
    "    df: The DataFrame to process.\n",
    "    column_index (int, optional): The index of the column to process. If None, all columns are processed.\n",
    "\n",
    "    Returns:\n",
    "    The DataFrame with inaccurate data fixed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If column index is not provided, process all columns\n",
    "        if column_index is None:\n",
    "            for col in df.columns:\n",
    "                try:\n",
    "                    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                        # Detect outliers\n",
    "                        lower_bound = df[col].quantile(0.01)\n",
    "                        upper_bound = df[col].quantile(0.99)\n",
    "                        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "                        if not outliers.empty:\n",
    "                            print(f\"Inaccurate data found in numeric column '{col}': {outliers.shape[0]} outliers.\")\n",
    "                            # Replace outliers with the median\n",
    "                            df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = df[col].median()\n",
    "                            print(f\"Outliers in '{col}' have been replaced with the median value.\")\n",
    "                    \n",
    "                    elif pd.api.types.is_string_dtype(df[col]):\n",
    "                        # Detect and fix inconsistencies\n",
    "                        inconsistent = df[col].str.contains('error', case=False, na=False)\n",
    "                        if inconsistent.any():\n",
    "                            print(f\"Inaccurate data found in string column '{col}': {inconsistent.sum()} potential errors.\")\n",
    "                            # Replace 'error' with 'Unknown'\n",
    "                            df[col] = df[col].str.replace('error', 'Unknown', case=False)\n",
    "                            print(f\"'Error' entries in '{col}' have been replaced with 'Unknown'.\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while checking or fixing column '{col}': {e}\")\n",
    "\n",
    "        # If column index is provided, process only that column\n",
    "        else:\n",
    "            if column_index < 0 or column_index >= len(df.columns):\n",
    "                print(f\"Column index {column_index} is out of range. Please provide a valid index.\")\n",
    "                return df\n",
    "            \n",
    "            col = df.columns[column_index]\n",
    "            try:\n",
    "                if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                    # Detect outliers\n",
    "                    lower_bound = df[col].quantile(0.01)\n",
    "                    upper_bound = df[col].quantile(0.99)\n",
    "                    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "                    if not outliers.empty:\n",
    "                        print(f\"Inaccurate data found in numeric column '{col}': {outliers.shape[0]} outliers.\")\n",
    "                        # Replace outliers with the median\n",
    "                        df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), col] = df[col].median()\n",
    "                        print(f\"Outliers in '{col}' have been replaced with the median value.\")\n",
    "                \n",
    "                elif pd.api.types.is_string_dtype(df[col]):\n",
    "                    # Detect and fix inconsistencies\n",
    "                    inconsistent = df[col].str.contains('error', case=False, na=False)\n",
    "                    if inconsistent.any():\n",
    "                        print(f\"Inaccurate data found in string column '{col}': {inconsistent.sum()} potential errors.\")\n",
    "                        # Replace 'error' with 'Unknown'\n",
    "                        df[col] = df[col].str.replace('error', 'Unknown', case=False)\n",
    "                        print(f\"'Error' entries in '{col}' have been replaced with 'Unknown'.\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while checking or fixing column '{col}': {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data accuracy checking: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to remove special characters and extra white spaces based on column index\n",
    "def show_text_columns(df):\n",
    "    \"\"\"Display the indices and names of columns that contain text data.\"\"\"\n",
    "    text_columns = [index for index, dtype in enumerate(df.dtypes) if pd.api.types.is_string_dtype(dtype) or pd.api.types.is_object_dtype(dtype)]\n",
    "    \n",
    "    if text_columns:\n",
    "        print(\"Text columns available for cleaning:\")\n",
    "        for index in text_columns:\n",
    "            print(f\"Index: {index}, Column Name: {df.columns[index]}\")\n",
    "    else:\n",
    "        print(\"No text columns found in the dataset.\")\n",
    "    \n",
    "    return text_columns\n",
    "\n",
    "def clean_text_columns_by_index(df, column_indices):\n",
    "    try:\n",
    "        # Ensure column_indices is a list even if a single index is provided\n",
    "        if isinstance(column_indices, int):\n",
    "            column_indices = [column_indices]\n",
    "        \n",
    "        for index in column_indices:\n",
    "            try:\n",
    "                if index < len(df.columns):  # Check if index is within the range of columns\n",
    "                    # Convert the entire column to string type before cleaning\n",
    "                    df.iloc[:, index] = df.iloc[:, index].astype(str)\n",
    "\n",
    "                    # Store original column values for comparison\n",
    "                    original_column = df.iloc[:, index].copy()\n",
    "\n",
    "                    # Clean the text\n",
    "                    df.iloc[:, index] = df.iloc[:, index].str.replace(r'[^A-Za-z0-9\\s]', '', regex=True)  # Remove special characters\n",
    "                    df.iloc[:, index] = df.iloc[:, index].str.strip()  # Remove leading and trailing white spaces\n",
    "                    df.iloc[:, index] = df.iloc[:, index].str.replace(r'\\s+', ' ', regex=True)  # Remove extra white spaces\n",
    "\n",
    "                    # Check and print if any changes were made\n",
    "                    changes_made = (original_column != df.iloc[:, index]).sum()\n",
    "                    if changes_made > 0:\n",
    "                        print(f\"Column at index {index} has been cleaned. {changes_made} rows were modified.\")\n",
    "                    else:\n",
    "                        print(f\"Column at index {index} was already clean. No changes were made.\")\n",
    "                        \n",
    "                    # Optionally, show a before and after comparison for the first few rows\n",
    "                    print(\"\\nSample of changes:\")\n",
    "                    comparison_df = pd.DataFrame({\n",
    "                        'Original': original_column.head(5),\n",
    "                        'Cleaned': df.iloc[:, index].head(5)\n",
    "                    })\n",
    "                    print(comparison_df)\n",
    "                else:\n",
    "                    print(f\"Index {index} is out of range.\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while cleaning text in column at index {index}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during text cleaning: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function to format phone numbers\n",
    "def is_valid_phone_number(number):\n",
    "    # Remove non-numeric characters except '+'\n",
    "    number = re.sub(r'[^\\d+]', '', number)\n",
    "    \n",
    "    # Normalize country code\n",
    "    if number.startswith('00'):\n",
    "        number = '+' + number[2:]\n",
    "    \n",
    "    # Validate length: local numbers (10 digits) or international numbers (13 digits)\n",
    "    if len(number) == 10 or (len(number) == 13 and number.startswith('+')):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def format_phone_number(phone):\n",
    "    # Split phone numbers if separated by common delimiters\n",
    "    phone_numbers = re.split(r'[\\/,;or\\s]+', phone)\n",
    "    \n",
    "    formatted_numbers = []\n",
    "    for number in phone_numbers:\n",
    "        # Remove non-numeric characters except '+'\n",
    "        number = re.sub(r'[^\\d+]', '', number)\n",
    "        \n",
    "        # Normalize country code\n",
    "        if number.startswith('00'):\n",
    "            number = '+' + number[2:]\n",
    "        \n",
    "        # Validate and add formatted number to the list\n",
    "        if is_valid_phone_number(number):\n",
    "            formatted_numbers.append(number)\n",
    "    \n",
    "    # Return both primary and alternate numbers, if available\n",
    "    if len(formatted_numbers) > 1:\n",
    "        return formatted_numbers[0], ', '.join(formatted_numbers[1:])\n",
    "    elif formatted_numbers:\n",
    "        return formatted_numbers[0], None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "def process_phone_numbers(df, phone_col_index):\n",
    "    phone_col_name = df.columns[phone_col_index]\n",
    "    \n",
    "    # Apply formatting function to the phone number column\n",
    "    df[['Primary Number', 'Alternate Number']] = df[phone_col_name].astype(str).apply(lambda x: pd.Series(format_phone_number(x)))\n",
    "    \n",
    "    # Drop the original phone number column\n",
    "    df = df.drop(columns=[phone_col_name])\n",
    "    \n",
    "    # Return the DataFrame with the new columns\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# Function to format names into first, middle, last name by column index\n",
    "def format_names(df, name_column_index):\n",
    "    try:\n",
    "        # Ensure the column index is within range\n",
    "        if name_column_index < 0 or name_column_index >= len(df.columns):\n",
    "            print(f\"Column index {name_column_index} is out of range. Please provide a valid index.\")\n",
    "            return df\n",
    "        \n",
    "        name_column = df.columns[name_column_index]\n",
    "\n",
    "        # Check if the column exists and is of string type\n",
    "        if name_column not in df.columns or not pd.api.types.is_string_dtype(df[name_column]):\n",
    "            print(f\"Column '{name_column}' either does not exist or is not a string type.\")\n",
    "            return df\n",
    "        \n",
    "        # Extract first, middle, and last names from the name column\n",
    "        df[['First Name', 'Middle Name', 'Last Name']] = df[name_column].str.extract(r'^(\\w+)\\s+(\\w+)?\\s*(\\w*)$', expand=True)\n",
    "        \n",
    "        # Handle missing middle names by setting them to NaN\n",
    "        df['Middle Name'] = df['Middle Name'].replace('', np.nan)\n",
    "\n",
    "        print(f\"Names in column '{name_column}' have been formatted into 'First Name', 'Middle Name', and 'Last Name'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while formatting names in column index {name_column_index}: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Function to format date columns by column index\n",
    "def format_date_columns(df):\n",
    "    try:\n",
    "        # Identify all columns with date-like data\n",
    "        date_columns = df.select_dtypes(include=[pd.DatetimeTZDtype, 'datetime64[ns]']).columns\n",
    "        \n",
    "        if date_columns.empty:\n",
    "            print(\"No date columns found in the dataset.\")\n",
    "            return df\n",
    "        \n",
    "        print(\"\\nDate columns identified:\")\n",
    "        for index, col in enumerate(date_columns):\n",
    "            print(f\"{index}: {col}\")\n",
    "\n",
    "        # Ask user to select columns to format\n",
    "        indices = input(\"Enter the indices of the date columns to format, separated by commas: \").strip().split(',')\n",
    "        indices = [int(index.strip()) for index in indices if index.strip().isdigit()]\n",
    "        \n",
    "        # Validate indices\n",
    "        invalid_indices = [i for i in indices if i >= len(date_columns) or i < 0]\n",
    "        if invalid_indices:\n",
    "            print(f\"Invalid indices provided: {', '.join(map(str, invalid_indices))}. These indices will be ignored.\")\n",
    "        \n",
    "        valid_indices = [i for i in indices if i < len(date_columns) and i >= 0]\n",
    "        if not valid_indices:\n",
    "            print(\"No valid indices provided. No columns will be formatted.\")\n",
    "            return df\n",
    "\n",
    "        # Extract selected date columns\n",
    "        selected_date_columns = [date_columns[i] for i in valid_indices]\n",
    "\n",
    "        # Ask user for the desired date format\n",
    "        date_format = input(\"Enter the desired date format (e.g., 'YYYY-MM-DD', 'MM/DD/YYYY', etc.): \").strip()\n",
    "        \n",
    "        if not date_format:\n",
    "            print(\"No date format provided. Skipping date formatting.\")\n",
    "            return df\n",
    "        \n",
    "        # Apply the format to each selected date column\n",
    "        for col in selected_date_columns:\n",
    "            df[col] = df[col].dt.strftime(date_format)\n",
    "            print(f\"Column '{col}' formatted to '{date_format}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while formatting date columns: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#Function to format address\n",
    "def parse_address(address):\n",
    "    # Split the address into components\n",
    "    address_parts = address.split(',')\n",
    "\n",
    "    # Initialize the components\n",
    "    house_number, area, city = None, None, None\n",
    "    \n",
    "    # Extract house number (assuming it's the first part)\n",
    "    if len(address_parts) > 0:\n",
    "        first_part = address_parts[0].strip()\n",
    "        # Check if the first part contains only digits\n",
    "        if first_part.isdigit():\n",
    "            house_number = first_part\n",
    "        else:\n",
    "            house_number = None\n",
    "\n",
    "    # Extract area (everything between house number and city)\n",
    "    if len(address_parts) > 1:\n",
    "        area = ', '.join([part.strip() for part in address_parts[1:-1]])\n",
    "    \n",
    "    # Extract city (assuming it's the last part)\n",
    "    if len(address_parts) > 2:\n",
    "        city = address_parts[-1].strip()\n",
    "\n",
    "    return house_number, area, city\n",
    "\n",
    "def process_addresses(df, address_col_index):\n",
    "    address_col_name = df.columns[address_col_index]\n",
    "    \n",
    "    # Apply address parsing function\n",
    "    df[['House Number', 'Area', 'City']] = df[address_col_name].astype(str).apply(lambda x: pd.Series(parse_address(x)))\n",
    "    \n",
    "    # Drop the original address column if needed\n",
    "    df = df.drop(columns=[address_col_name])\n",
    "    \n",
    "    # Return the DataFrame with the new columns\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# Function to standardize text data\n",
    "def standardize_text_data(df, remove_special_chars=False):\n",
    "    try:\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            df[col] = df[col].str.lower().str.strip()  # Convert to lowercase and remove extra spaces\n",
    "            if remove_special_chars:\n",
    "                df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)  # Remove special characters\n",
    "            print(f\"Text data in column '{col}' standardized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while standardizing text data: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to normalize numeric data\n",
    "def normalize_numeric_data(df):\n",
    "    try:\n",
    "        # Get the normalization method from the user\n",
    "        method = input(\"Enter the normalization method ('min-max' or 'z-score'): \").strip().lower()\n",
    "        \n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            if method == 'min-max':\n",
    "                df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "                print(f\"Column '{col}' normalized using Min-Max scaling.\")\n",
    "            elif method == 'z-score':\n",
    "                df[col] = (df[col] - df[col].mean()) / df[col].std()\n",
    "                print(f\"Column '{col}' normalized using Z-score standardization.\")\n",
    "            else:\n",
    "                print(f\"Normalization method '{method}' is not recognized. Skipping column '{col}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while normalizing numeric data: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#Function to rename column name\n",
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    Prompts the user to rename columns in the given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame whose columns are to be renamed.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with renamed columns.\n",
    "    \"\"\"\n",
    "    # Display the current column names\n",
    "    print(\"Current columns in the DataFrame:\")\n",
    "    print(list(df.columns))\n",
    "   \n",
    "    while True:\n",
    "        # Prompt the user for the column to rename\n",
    "        current_name = input(\"\\nEnter the current column name you want to rename (or type 'exit' to finish): \").strip()\n",
    "       \n",
    "        if current_name.lower() == 'exit':\n",
    "            break\n",
    "       \n",
    "        # Check if the column exists in the DataFrame\n",
    "        if current_name not in df.columns:\n",
    "            print(f\"Error: Column '{current_name}' does not exist in the DataFrame.\")\n",
    "            continue\n",
    "       \n",
    "        # Prompt the user for the new column name\n",
    "        new_name = input(f\"Enter the new name for column '{current_name}': \").strip()\n",
    "       \n",
    "        # Rename the column\n",
    "        df.rename(columns={current_name: new_name}, inplace=True)\n",
    "       \n",
    "        print(f\"Column '{current_name}' has been renamed to '{new_name}'.\")\n",
    "        print(\"Updated columns in the DataFrame:\")\n",
    "        print(list(df.columns))\n",
    "   \n",
    "    return df\n",
    "\n",
    "\n",
    "#Function to combine columns \n",
    "def get_columns_to_combine(df):\n",
    "    \"\"\"\n",
    "    Prompts the user for columns to combine and validates the columns.\n",
    "    \"\"\"\n",
    "    print(f\"Columns in the dataset: {list(df.columns)}\")\n",
    "    columns_to_combine = input(\"Enter the columns to combine (comma-separated): \").split(',')\n",
    "    columns_to_combine = [col.strip() for col in columns_to_combine]\n",
    "\n",
    "    missing_columns = [col for col in columns_to_combine if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Columns not found in DataFrame: {missing_columns}\")\n",
    "\n",
    "    return columns_to_combine\n",
    "\n",
    "def get_user_format(columns_to_combine):\n",
    "    \"\"\"\n",
    "    Prompts the user for the format and validates the number of placeholders.\n",
    "    \"\"\"\n",
    "    user_format = input(\"Enter the format for combining the columns (e.g., '{0} {1}'): \")\n",
    "   \n",
    "    placeholders = user_format.count('{')\n",
    "    if placeholders != len(columns_to_combine):\n",
    "        raise ValueError(f\"The format string requires {placeholders} placeholders, but {len(columns_to_combine)} columns were provided.\")\n",
    "\n",
    "    return user_format\n",
    "\n",
    "def combine_columns(df, columns_to_combine, new_column_name, user_format):\n",
    "    \"\"\"\n",
    "    Combines the specified columns in the DataFrame using the user-defined format.\n",
    "    \"\"\"\n",
    "    df[new_column_name] = df[columns_to_combine].apply(lambda row: user_format.format(*row), axis=1)\n",
    "    return df\n",
    "\n",
    "def user_combine_columns(df):\n",
    "    \"\"\"\n",
    "    Handles the user interaction for combining columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        columns_to_combine = get_columns_to_combine(df)\n",
    "        user_format = get_user_format(columns_to_combine)\n",
    "        new_column_name = input(\"Enter the name for the new combined column: \").strip()\n",
    "\n",
    "        df = combine_columns(df, columns_to_combine, new_column_name, user_format)\n",
    "        print(f\"Columns combined successfully into '{new_column_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Appliying filter for column by using Condition \n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def filter_dataset_from_file(df):\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    \n",
    "    while True:\n",
    "        # Get column to filter\n",
    "        column_name = input(\"Enter the column name you want to filter by (or 'exit' to finish): \")\n",
    "        if column_name.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        # Ensure the column exists in the DataFrame\n",
    "        if column_name not in df.columns:\n",
    "            print(f\"Column '{column_name}' not found. Please enter a valid column name.\")\n",
    "            continue\n",
    "        \n",
    "        # Get filter condition from user\n",
    "        condition = input(\"Enter the condition (e.g., > 50000, <= 55000): \").strip()\n",
    "        \n",
    "        # Escape backticks in column names\n",
    "        column_name = f\"`{column_name}`\"\n",
    "        \n",
    "        try:\n",
    "            # Dynamically apply the filter using query\n",
    "            df = df.query(f\"{column_name} {condition}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in applying filter: {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Filtered DataFrame:\\n{df}\\n\")\n",
    "        \n",
    "        another_filter = input(\"Do you want to apply another filter? (yes/no): \").strip().lower()\n",
    "        if another_filter != 'yes':\n",
    "            break\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#Function to convert data type from one format to another\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def convert_data_types(df: pd.DataFrame, conversions: Dict[str, str]) -> pd.DataFrame:\n",
    "    try:\n",
    "        # Display available columns\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        \n",
    "        while True:\n",
    "            # Get the column name from the user\n",
    "            column_name = input(\"Enter the column name you want to convert (or 'exit' to finish): \")\n",
    "            if column_name.lower() == 'exit':\n",
    "                break\n",
    "\n",
    "            # Ensure the column exists in the DataFrame\n",
    "            if column_name not in df.columns:\n",
    "                print(f\"Column '{column_name}' not found. Please enter a valid column name.\")\n",
    "                continue\n",
    "            \n",
    "            # Show current data type\n",
    "            current_dtype = df[column_name].dtype\n",
    "            print(f\"Current data type of '{column_name}' is: {current_dtype}\")\n",
    "            \n",
    "            # Get the desired data type from the user\n",
    "            target_dtype = input(\"Enter the data type you want to convert to (e.g., 'int', 'float', 'str', 'datetime'): \").strip()\n",
    "\n",
    "            try:\n",
    "                # Perform the conversion\n",
    "                df[column_name] = df[column_name].astype(target_dtype)\n",
    "                conversions[column_name] = target_dtype\n",
    "                print(f\"Converted '{column_name}' to {target_dtype}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting '{column_name}' to {target_dtype}: {e}\")\n",
    "                continue\n",
    "\n",
    "            another_conversion = input(\"Do you want to convert another column? (yes/no): \").strip().lower()\n",
    "            if another_conversion != 'yes':\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#Function for summary statistics\n",
    "import pandas as pd\n",
    "def generate_summary_statistics(df: pd.DataFrame):\n",
    "    try:\n",
    "        # Display the list of columns to the user\n",
    "        print(\"Columns in the dataset:\")\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            print(f\"{idx}: {col}\")\n",
    "        \n",
    "        # Ask the user to input the column index they want to analyze\n",
    "        col_idx = int(input(\"Enter the column index you want to see summary statistics for: \"))\n",
    "        \n",
    "        # Validate the user's input\n",
    "        if col_idx < 0 or col_idx >= len(df.columns):\n",
    "            raise ValueError(\"Invalid column index. Please enter a valid index from the list.\")\n",
    "        \n",
    "        # Get the column name based on the index\n",
    "        col_name = df.columns[col_idx]\n",
    "        \n",
    "        # Generate and display summary statistics for the chosen column\n",
    "        summary_stats = df[col_name].describe(include='all')\n",
    "        print(f\"\\nSummary Statistics for '{col_name}':\")\n",
    "        print(summary_stats)\n",
    "        \n",
    "    except ValueError as ve:\n",
    "        print(f\"Value Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "#Function to aggregate\n",
    "import pandas as pd\n",
    "def apply_aggregation(df: pd.DataFrame, group_by: str):\n",
    "    try:\n",
    "        # Check if the group_by column exists in the dataframe\n",
    "        if group_by not in df.columns:\n",
    "            raise ValueError(f\"Column '{group_by}' does not exist in the DataFrame.\")\n",
    "        \n",
    "        # Display the list of columns to the user\n",
    "        print(\"Columns in the dataset:\")\n",
    "        for idx, col in enumerate(df.columns):\n",
    "            print(f\"{idx}: {col}\")\n",
    "        \n",
    "        # Ask the user to select columns they want to aggregate (by index)\n",
    "        selected_columns = input(\"Enter the column indices you want to aggregate (comma-separated): \")\n",
    "        selected_columns = [df.columns[int(idx)] for idx in selected_columns.split(',')]\n",
    "        \n",
    "        # Display available aggregation functions\n",
    "        available_functions = {\n",
    "            'sum': 'Sum',\n",
    "            'mean': 'Average',\n",
    "            'count': 'Count',\n",
    "            'min': 'Minimum',\n",
    "            'max': 'Maximum'\n",
    "        }\n",
    "        print(\"Available aggregation functions:\")\n",
    "        for key, value in available_functions.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Ask the user to input aggregation functions (comma-separated)\n",
    "        agg_functions = input(\"Enter the aggregation functions you want to apply (comma-separated): \").lower().split(',')\n",
    "        \n",
    "        # Build the aggregation dictionary\n",
    "        aggregation_dict = {col: agg_functions for col in selected_columns}\n",
    "        \n",
    "        # Perform aggregation\n",
    "        aggregated_df = df.groupby(group_by).agg(aggregation_dict)\n",
    "        \n",
    "        # Display the result\n",
    "        print(\"\\nAggregated Data:\")\n",
    "        print(aggregated_df)\n",
    "        \n",
    "        return aggregated_df\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        print(f\"Value Error: {ve}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "#Function to select row by value\n",
    "import pandas as pd\n",
    "def select_row_by_value(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Select rows in the dataset where the value in a specified column matches the user's input.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to search within.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prompt user for column and value\n",
    "    column_name = input(\"Please enter the name of the column you want to search in: \").strip()\n",
    "    input_value = input(\"Please enter the value to search for: \").strip()\n",
    "\n",
    "    # Parse the input value to its appropriate data type\n",
    "    parsed_value = parse_value(input_value)\n",
    "\n",
    "    # Select rows where the column value matches the parsed input value\n",
    "    if column_name in df.columns:\n",
    "        selected_rows = df[df[column_name] == parsed_value]\n",
    "\n",
    "        if selected_rows.empty:\n",
    "            print(f\"No matching rows found for value: {parsed_value}\")\n",
    "        else:\n",
    "            print(\"Matching rows found:\")\n",
    "            print(selected_rows)\n",
    "    else:\n",
    "        print(f\"Column '{column_name}' does not exist in the dataset.\")\n",
    "        \n",
    "def parse_value(input_value):\n",
    "    \"\"\"\n",
    "    Parses the input value to its appropriate data type.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_value (str): The input value as a string.\n",
    "    \n",
    "    Returns:\n",
    "    - Parsed value (int, float, str): The value converted to the most appropriate data type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if '.' in input_value:\n",
    "            return float(input_value)\n",
    "        else:\n",
    "            return int(input_value)\n",
    "    except ValueError:\n",
    "        return input_value\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def sort_column(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Sorts the DataFrame based on a specified column and order provided by the user.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The DataFrame to sort.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: The sorted DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prompt user for column to sort by\n",
    "    column_name = input(\"Please enter the name of the column to sort by: \").strip()\n",
    "    \n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' does not exist in the dataset.\")\n",
    "    \n",
    "    # Prompt user for sort order\n",
    "    sort_order = input(\"Enter 'asc' for ascending order or 'desc' for descending order: \").strip().lower()\n",
    "\n",
    "    if sort_order not in ['asc', 'desc']:\n",
    "        raise ValueError(\"Invalid sort order. Please enter 'asc' for ascending or 'desc' for descending.\")\n",
    "    \n",
    "    ascending = True if sort_order == 'asc' else False\n",
    "\n",
    "    # Sort the dataset\n",
    "    sorted_dataset = df.sort_values(by=column_name, ascending=ascending)\n",
    "    \n",
    "    # Display the sorted DataFrame\n",
    "    print(f\"\\nSorted DataFrame by '{column_name}' in {'ascending' if ascending else 'descending'} order:\")\n",
    "    print(sorted_dataset)\n",
    "    \n",
    "    return sorted_dataset\n",
    "\n",
    "\n",
    "#Function to format currency\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def is_valid_currency_symbol(symbol):\n",
    "    \"\"\"Simple validation to check if the symbol is a valid currency type.\"\"\"\n",
    "    return len(symbol) in [1, 2, 3] and symbol.isalpha()\n",
    "\n",
    "def convert_currency(amount, conversion_rate):\n",
    "    \"\"\"Converts the given amount using the provided conversion rate.\"\"\"\n",
    "    try:\n",
    "        return amount * conversion_rate\n",
    "    except TypeError:\n",
    "        print(f\"Error: Unable to convert amount '{amount}' with rate '{conversion_rate}'. Ensure the amount is numeric.\")\n",
    "        return amount\n",
    "\n",
    "def extract_currency_and_amount(value):\n",
    "    \"\"\"Extracts currency symbol and amount from a combined string like 'INR 30000' or '$100'.\"\"\"\n",
    "    currency_match = re.match(r\"([^\\d\\s]+)\", value)\n",
    "    amount_match = re.search(r\"(\\d+(\\.\\d+)?)\", value)\n",
    "   \n",
    "    if currency_match and amount_match:\n",
    "        currency = currency_match.group(0).strip()\n",
    "        amount = float(amount_match.group(0))\n",
    "        return currency, amount\n",
    "    else:\n",
    "        print(f\"Warning: Could not extract currency and amount from '{value}'.\")\n",
    "        return None, None\n",
    "\n",
    "def process_currency_info(df):\n",
    "    # Extract currency and amount from the 'Currency' column\n",
    "    df[['Currency', 'Amount']] = df['Currency'].apply(lambda x: pd.Series(extract_currency_and_amount(x)))\n",
    "   \n",
    "    while True:\n",
    "        print(\"\\nMenu:\")\n",
    "        print(\"1. Display basic info (count of each value in a particular column)\")\n",
    "        print(\"2. Convert currency symbol alone\")\n",
    "        print(\"3. Convert currency value (e.g., EUR to INR) and update the symbol\")\n",
    "        choice = input(\"Enter your choice (1/2/3) or 'q' to quit: \").strip().lower()\n",
    "\n",
    "        if choice == 'q':\n",
    "            break\n",
    "\n",
    "        if choice == '1':\n",
    "            # Display unique currency count\n",
    "            currency_count = df['Currency'].nunique()\n",
    "            print(f\"\\nThere are {currency_count} unique currencies in the dataset.\")\n",
    "\n",
    "            # Display the count of each currency type\n",
    "            currency_type_count = df['Currency'].value_counts()\n",
    "            print(\"\\nCurrency counts:\")\n",
    "            print(currency_type_count)\n",
    "\n",
    "        elif choice == '2':\n",
    "            # Convert currency symbol alone\n",
    "            old_currency = input(\"Enter the current currency symbol you want to change: \").strip().upper()\n",
    "            if not is_valid_currency_symbol(old_currency):\n",
    "                print(\"Error: Invalid currency symbol.\")\n",
    "                continue\n",
    "\n",
    "            new_currency = input(\"Enter the new currency symbol: \").strip().upper()\n",
    "            if not is_valid_currency_symbol(new_currency):\n",
    "                print(\"Error: Invalid currency symbol.\")\n",
    "                continue\n",
    "\n",
    "            df['Currency'] = df['Currency'].replace(old_currency, new_currency)\n",
    "            print(f\"\\nUpdated currency symbol '{old_currency}' to '{new_currency}'.\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            # Convert currency value and update the symbol\n",
    "            old_currency = input(\"Enter the current currency symbol you want to change: \").strip().upper()\n",
    "            if not is_valid_currency_symbol(old_currency):\n",
    "                print(\"Error: Invalid currency symbol.\")\n",
    "                continue\n",
    "\n",
    "            new_currency = input(\"Enter the new currency symbol: \").strip().upper()\n",
    "            if not is_valid_currency_symbol(new_currency):\n",
    "                print(\"Error: Invalid currency symbol.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                conversion_rate = float(input(f\"Enter the conversion rate from {old_currency} to {new_currency}: \").strip())\n",
    "            except ValueError:\n",
    "                print(\"Error: Invalid conversion rate. Please enter a numeric value.\")\n",
    "                continue\n",
    "\n",
    "            # Convert and aggregate the amount for the new currency\n",
    "            df.loc[df['Currency'] == old_currency, 'Amount'] *= conversion_rate\n",
    "            df['Currency'] = df['Currency'].replace(old_currency, new_currency)\n",
    "\n",
    "            # Aggregate the amounts by currency\n",
    "            df = df.groupby('Currency').agg({'Amount': 'sum'}).reset_index()\n",
    "            print(f\"\\nConverted amounts from {old_currency} to {new_currency} and updated the currency symbol.\")\n",
    "            print(\"\\nUpdated DataFrame:\")\n",
    "            print(df)\n",
    "\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter 1, 2, 3, or 'q' to quit.\")\n",
    " # Return the updated DataFrame if needed\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = None\n",
    "def clean_dataset():\n",
    "    df = load_dataset()\n",
    "    if df is None:\n",
    "        print(\"Failed to load the dataset. Exiting the data cleaning process.\")\n",
    "        return\n",
    "\n",
    "    backup_df = create_backup(df)\n",
    "    print(\"Backup of the original dataset created.\")\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"1. Basic Operations\")\n",
    "        print(\"2. Column-wise Operations\")\n",
    "        print(\"3. Show DataFrame\")\n",
    "        print(\"4. Exit\")\n",
    "\n",
    "        choice = input(\"Choose an option: \").strip()\n",
    "        \n",
    "        if choice == '1':\n",
    "            while True:\n",
    "                print(\"\\nBasic Operations:\")\n",
    "                print(\"1. Display basic info\")\n",
    "                print(\"2. Display head and tail of data\")\n",
    "                print(\"3. Display Summary Statistics\")\n",
    "                print(\"4. Check missing values\")\n",
    "                print(\"5. Check inconsistencies\")\n",
    "                print(\"6. Return to Main Menu\")\n",
    "                print(\"7. Exit\")\n",
    "\n",
    "                basic_choice = input(\"Choose an option: \").strip()\n",
    "\n",
    "                if basic_choice == '1':\n",
    "                    print(df)\n",
    "                    display_basic_info(df)\n",
    "                elif basic_choice == '2':  # Corrected from 'choice' to 'basic_choice'\n",
    "                    display_choice = input(\"Would you like to see the first few rows (head), the last few rows (tail), or both? (head/tail/both): \").strip().lower()\n",
    "                    show_head_and_tail(df, rows=5, display_choice=display_choice)\n",
    "                elif basic_choice == '3':\n",
    "                    generate_summary_statistics(df)\n",
    "                elif basic_choice == '4':\n",
    "                    check_missing_values(df)\n",
    "                elif basic_choice == '5':\n",
    "                    check_inconsistencies(df)\n",
    "                elif basic_choice == '6':\n",
    "                    break\n",
    "                elif basic_choice == '7':\n",
    "                    print(\"Exiting the data cleaning process.\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(\"Invalid choice. Please select a valid option.\")\n",
    "        \n",
    "        elif choice == '2':\n",
    "            while True:\n",
    "                print(\"\\nColumn-wise Operations:\")\n",
    "                print(\"1. Remove columns\")\n",
    "                print(\"2. Drop duplicates\")\n",
    "                print(\"3. Drop missing rows\")\n",
    "                print(\"4. Handle missing values\")\n",
    "                print(\"5. Interpolate missing values\")\n",
    "                print(\"6. Detect and fix inaccurate data\")\n",
    "                print(\"7. Remove special characters and extra white spaces\")\n",
    "                print(\"8. Format phone numbers\")\n",
    "                print(\"9. Format names into first, middle, last name\")\n",
    "                print(\"10. Format dates\")\n",
    "                print(\"11. Format addresses\")\n",
    "                print(\"12. Standardize text data\")\n",
    "                print(\"13. Normalize numeric data\")\n",
    "                print(\"14. Renaming Columns Name\")\n",
    "                print(\"15. Concatenate Column \")\n",
    "                print(\"16. Filter Condition For numeric data\")\n",
    "                print(\"17. Convert Data Types\")\n",
    "                print(\"18. Apply Aggregation Functions\")\n",
    "                print(\"19. Search in row by value\")\n",
    "                print(\"20. Sort column\")\n",
    "                print(\"21. Foramt Currency\")\n",
    "                print(\"21. Return to Main Menu\")\n",
    "                print(\"22. Exit\")\n",
    "\n",
    "                column_choice = input(\"Choose an option: \").strip()\n",
    "\n",
    "                if column_choice == '1':\n",
    "                    df = remove_columns_by_index(df)\n",
    "                elif column_choice == '2':\n",
    "                    df = check_and_handle_duplicates(df)\n",
    "                elif column_choice == '3':\n",
    "                    columns_to_check = input(\"Enter the column indices (comma-separated) to check for missing values: \").strip()\n",
    "                    columns_to_check = [df.columns[int(index)] for index in columns_to_check.split(',')]\n",
    "                    df = drop_missing_rows(df, columns_to_check)\n",
    "                elif column_choice == '4':\n",
    "                    df = handle_missing_values(df)\n",
    "                elif column_choice == '5':\n",
    "                    df = interpolate_missing_values(df)\n",
    "                elif column_choice == '6':\n",
    "                    column_index = input(\"Enter the column index to check for inaccuracies (or leave blank to check all columns): \").strip()\n",
    "                    column_index = int(column_index) if column_index else None\n",
    "                    df = detect_and_fix_inaccurate_data(df, column_index)\n",
    "                elif column_choice == '7':\n",
    "                    text_columns = show_text_columns(df)\n",
    "                    if text_columns:\n",
    "                        column_indices = list(map(int, input(\"Enter the column indices for text cleaning (comma-separated): \").split(',')))\n",
    "                        df = clean_text_columns_by_index(df, column_indices)\n",
    "                    else:\n",
    "                        print(\"No text columns available for cleaning.\")\n",
    "                elif column_choice == '8':\n",
    "                    phone_col_index = int(input(\"Enter the index of the phone number column: \"))\n",
    "                    df = process_phone_numbers(df, phone_col_index)\n",
    "                elif column_choice == '9':\n",
    "                    name_column_index = int(input(\"Enter the column index for names: \").strip())\n",
    "                    df = format_names(df, name_column_index)\n",
    "                elif column_choice == '10':\n",
    "                    df = format_date_columns(df)\n",
    "                elif column_choice == '11':\n",
    "                    address_column_index = int(input(\"Enter the column index for addresses: \").strip())\n",
    "                    df = process_addresses(df, address_column_index)\n",
    "                elif column_choice == '12':\n",
    "                    df = standardize_text_data(df, remove_special_chars=False)\n",
    "                elif column_choice == '13':\n",
    "                    df = normalize_numeric_data(df)\n",
    "                elif column_choice == '14':\n",
    "                    df = rename_columns(df)\n",
    "                elif column_choice == '15':\n",
    "                    df = user_combine_columns(df)\n",
    "                elif column_choice == '16':\n",
    "                    df = filter_dataset_from_file(df)\n",
    "                elif column_choice == '17':    \n",
    "                    df = convert_data_types(df, conversions={})\n",
    "                elif column_choice == '18':    \n",
    "                    group_by_column = input(\"Enter the column name to group by: \")\n",
    "                    apply_aggregation(df, group_by_column)\n",
    "                elif column_choice == '19':    \n",
    "                    select_row_by_value(df)\n",
    "                elif column_choice == '20':    \n",
    "                    sorted_df = sort_column(df)\n",
    "                elif column_choice == '21':    \n",
    "                    updated_df = process_currency_info(df)\n",
    "                elif column_choice == '22':\n",
    "                    break\n",
    "                elif column_choice == '23':\n",
    "                    print(\"Exiting the data cleaning process.\")\n",
    "                    return\n",
    "                else:\n",
    "                    print(\"Invalid choice. Please select a valid option.\")\n",
    "                \n",
    "                \n",
    "        elif choice == '3':\n",
    "            print(\"\\nCurrent DataFrame:\")\n",
    "            print(df.head())\n",
    "        \n",
    "        elif choice == '4':\n",
    "            print(\"Exiting the data cleaning process.\")\n",
    "            break\n",
    "        \n",
    "        else:\n",
    "            print(\"Invalid choice. Please select a valid option.\")\n",
    "\n",
    "# Run the data cleaning process\n",
    "clean_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e6230-9202-4f07-893a-e7a368031eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1faddb2-b0d7-4402-8b3a-f5b696074370",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
